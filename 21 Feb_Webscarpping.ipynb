{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n\nWeb scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. \n\n1. Price Monitoring\nWeb Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n\n2. Market Research\nWeb scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future. \n\n3. News Monitoring\nWeb scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q2. What are the different methods used for Web Scraping?\n\nBrowser extensions Web Scrapers are extensions that can be added to your browser. These are easy to run as they are integrated with your browser, but at the same time, they are also limited because of this. Any advanced features that are outside the scope of your browser are impossible to run on Browser extension Web Scrapers. But Software Web Scrapers donâ€™t have these limitations as they can be downloaded and installed on your computer. These are more complex than Browser web scrapers, but they also have advanced features that are not limited by the scope of your browser.\n\nCloud Web Scrapers run on the cloud, which is an off-site server mostly provided by the company that you buy the scraper from. These allow your computer to focus on other tasks as the computer resources are not required to scrape data from websites. Local Web Scrapers, on the other hand, run on your computer using local resources. So, if the Web scrapers require more CPU or RAM, then your computer will become slow and not be able to perform other tasks.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q3. What is Beautiful Soup? Why is it used?\n\nBeautiful Soup is a Python library for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping2. Beautiful Soup helps with isolating titles and links from webpages. It is a tool for web scraping that helps you clean up and parse the documents you have pulled down from the web3.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q4. Why is flask used in this Web Scraping project?\n\nFlask api is used in this project to show the results of the content we are searching.\nOn the web page will give the input and it will search the appropriate results from the scarpped content.basically it will work like google web search.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n\nCode pipeline\nBeanstalk\n\nuses:\nAWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software.\n\nWith Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.",
      "metadata": {}
    }
  ]
}